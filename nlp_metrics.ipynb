{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['video_name', 'video_id', 'pred_sentence', 'ref_sentences'])\n",
      "18 52\n",
      "29 54\n",
      "30 62\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('data/save_test_data_seq.pkl', 'rb') as f:\n",
    "    data_seq = pickle.load(f)\n",
    "len_data = len(data_seq)\n",
    "print(data_seq[0].keys())\n",
    "with open('data/save_test_data_mean.pkl', 'rb') as f:\n",
    "    data_mean = pickle.load(f)\n",
    "with open('data/save_test_data_single.pkl', 'rb') as f:\n",
    "    data_single = pickle.load(f)\n",
    "\n",
    "def fn(input):\n",
    "    for i in range(len_data):\n",
    "        input[i]['ref_sentences'] = data_seq[i]['ref_sentences']\n",
    "        assert input[i]['video_name'] == data_seq[i]['video_name']\n",
    "    \n",
    "fn(data_mean)\n",
    "fn(data_single)\n",
    "\n",
    "# print(data[100])\n",
    "for i in range(3):\n",
    "    print(len(data_seq[i]['pred_sentence']), len(data_seq[i]['ref_sentences']))\n",
    "    # print(data[i]['pred_sentence'])\n",
    "    # print(data[i]['ref_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CIDEr': 0.549278193725268, 'METEOR': 0.2970440063021677, 'BLEU': 0.5065889189044042, 'ROUGE': 0.6410289388579278}\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import itertools\n",
    "from evaluation.cider import Cider\n",
    "from evaluation.meteor import Meteor\n",
    "from evaluation.bleu import Bleu\n",
    "from evaluation.rouge import Rouge\n",
    "\n",
    "import numpy as np\n",
    "from evaluation.tokenizer import PTBTokenizer\n",
    "\n",
    "data = data_seq\n",
    "\n",
    "def compute_scores(gts, gen):\n",
    "    metrics = (Cider(), Meteor(), Bleu(), Rouge()) \n",
    "    all_score = {}\n",
    "    all_scores = {}\n",
    "    for metric in metrics:\n",
    "        score, scores = metric.compute_score(gts, gen)\n",
    "        all_score[str(metric)] = score\n",
    "        all_scores[str(metric)] = scores\n",
    "\n",
    "    n_bleu = len(all_score['BLEU'])\n",
    "    bleu_scores = all_score['BLEU']\n",
    "    all_score['BLEU'] = np.exp ( np.mean(np.log(bleu_scores)) )\n",
    "    return all_score, all_scores\n",
    "\n",
    "gen, gts = {}, {}\n",
    "\n",
    "len_data = len(data)\n",
    "for i in range(len_data):\n",
    "    gts_i, gen_i =  data[i]['ref_sentences'], data[i]['pred_sentence'] # gts_i: list, gen_i: str\n",
    "    video_id = data[i]['video_id']\n",
    "\n",
    "    # gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "    # print(gen_i)\n",
    "    gen[i] = [gen_i, ]\n",
    "    gts[i] = gts_i\n",
    "\n",
    "\n",
    "gts = PTBTokenizer.tokenize(gts)\n",
    "gen = PTBTokenizer.tokenize(gen)\n",
    "# print(gen[0])\n",
    "# print(gts[0])\n",
    "scores, _ = compute_scores(gts, gen)\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     gts_i, gen_i =  data[i]['ref_sentences'], data[i]['pred_sentence']\n",
    "#     video_id = data[i]['video_id']\n",
    "    \n",
    "#     # for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "#     gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "#     gen[video_id] = [gen_i, ]\n",
    "#     gts[video_id] = gts_i\n",
    "\n",
    "#     cider_fn = Cider()\n",
    "#     cider_fn.compute_score(gen, gts)\n",
    "#     # PTBTokenizer.tokenize(data[0]['pred_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, dataloader, text_field, exp_name, epoch):\n",
    "    import itertools\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "    with tqdm(desc='Epoch %d - evaluation' % e, unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, (images, caps_gt) in enumerate(iter(dataloader)):\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<|endoftext|>'], 5, out_size=1)\n",
    "            caps_gen = text_field.decode(out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                gts['%d_%d' % (it, i)] = gts_i\n",
    "            pbar.update()\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "\n",
    "\n",
    "    scores, _ = evaluation.compute_scores(gts, gen)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def evaluation(y_pred, y_true, max_n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_true: shape (B, len_seq) an iterable of candidate translations. Each translation is an iterable of tokens,\n",
    "        e.g., [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']];\n",
    "        \n",
    "        y_pred: prediction, in same format as y_true;\n",
    "\n",
    "        max_n: the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams, bigrams and trigram;\n",
    "\n",
    "    Return:\n",
    "        BLEU  METEOR  CIDEr   SPICE\n",
    "    \"\"\"\n",
    "    bleu_weights = [ 1 / max_n for _ in range(max_n) ]\n",
    "    bleu = bleu_score(y_pred, y_true, max_n=max_n, weights=bleu_weights)\n",
    "\n",
    "    return bleu\n",
    "\n",
    "    \n",
    "    meteor = Meteor()\n",
    "    meteor = meteor.compute_score(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    return bleu, meteor\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
    "candidate_corpus = [['My', 'big', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
    "\n",
    "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
    "\n",
    "\n",
    "a = evaluation(candidate_corpus, references_corpus, 2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import tarfile\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_from_url(url, path):\n",
    "    \"\"\"Download file, with logic (from tensor2tensor) for Google Drive\"\"\"\n",
    "    if 'drive.google.com' not in url:\n",
    "        print('Downloading %s; may take a few minutes' % url)\n",
    "        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        with open(path, \"wb\") as file:\n",
    "            file.write(r.content)\n",
    "        return\n",
    "    print('Downloading from Google Drive; may take a few minutes')\n",
    "    confirm_token = None\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, stream=True)\n",
    "    for k, v in response.cookies.items():\n",
    "        if k.startswith(\"download_warning\"):\n",
    "            confirm_token = v\n",
    "\n",
    "    if confirm_token:\n",
    "        url = url + \"&confirm=\" + confirm_token\n",
    "        response = session.get(url, stream=True)\n",
    "\n",
    "    chunk_size = 16 * 1024\n",
    "    with open(path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "METEOR_GZ_URL = 'http://aimagelab.ing.unimore.it/speaksee/data/meteor.tgz'\n",
    "METEOR_JAR = 'meteor-1.5.jar'\n",
    "\n",
    "class Meteor:\n",
    "    def __init__(self):\n",
    "        __file__ = '/users/yluo73/workspace/cv1430/DL2470/nlp_metrics.ipynb'\n",
    "        base_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        jar_path = os.path.join(base_path, METEOR_JAR)\n",
    "        gz_path = os.path.join(base_path, os.path.basename(METEOR_GZ_URL))\n",
    "        if not os.path.isfile(jar_path):\n",
    "            if not os.path.isfile(gz_path):\n",
    "                download_from_url(METEOR_GZ_URL, gz_path)\n",
    "            tar = tarfile.open(gz_path, \"r\")\n",
    "            tar.extractall(path=os.path.dirname(os.path.abspath(__file__)))\n",
    "            tar.close()\n",
    "            os.remove(gz_path)\n",
    "\n",
    "        self.meteor_cmd = ['java', '-jar', '-Xmx2G', METEOR_JAR, \\\n",
    "                '-', '-', '-stdio', '-l', 'en', '-norm']\n",
    "        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n",
    "                cwd=os.path.dirname(os.path.abspath(__file__)), \\\n",
    "                stdin=subprocess.PIPE, \\\n",
    "                stdout=subprocess.PIPE, \\\n",
    "                stderr=subprocess.PIPE)\n",
    "        # Used to guarantee thread safety\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "        scores = []\n",
    "\n",
    "        eval_line = 'EVAL'\n",
    "        self.lock.acquire()\n",
    "        for i in imgIds:\n",
    "            assert(len(res[i]) == 1)\n",
    "            stat = self._stat(res[i][0], gts[i])\n",
    "            eval_line += ' ||| {}'.format(stat)\n",
    "\n",
    "        self.meteor_p.stdin.write('{}\\n'.format(eval_line).encode())\n",
    "        self.meteor_p.stdin.flush()\n",
    "        for i in range(0,len(imgIds)):\n",
    "            scores.append(float(self.meteor_p.stdout.readline().strip()))\n",
    "        score = float(self.meteor_p.stdout.readline().strip())\n",
    "        self.lock.release()\n",
    "\n",
    "        return score, scores\n",
    "\n",
    "    def _stat(self, hypothesis_str, reference_list):\n",
    "        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n",
    "        hypothesis_str = hypothesis_str.replace('|||','').replace('  ',' ')\n",
    "        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n",
    "        self.meteor_p.stdin.write('{}\\n'.format(score_line).encode())\n",
    "        self.meteor_p.stdin.flush()\n",
    "        raw = self.meteor_p.stdout.readline().decode().strip()\n",
    "        numbers = [str(int(float(n))) for n in raw.split()]\n",
    "        return ' '.join(numbers)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.lock.acquire()\n",
    "        self.meteor_p.stdin.close()\n",
    "        self.meteor_p.kill()\n",
    "        self.meteor_p.wait()\n",
    "        self.lock.release()\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'METEOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f51b89450fd118e3f37cdb0cb45214767939e989c04a3784ad7014553c4fe783"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
